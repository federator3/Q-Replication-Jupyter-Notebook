{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='top'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  IBM Q Replication Monitoring Jupyter Library\n",
    "Author: Christian Lenke, IBM<br>\n",
    "Version: 2019-07-23"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Preface](#intro)\n",
    "* [Introduction](#intro)\n",
    "* [Individual Setup](#setup)\n",
    "<p>\n",
    "* [Installation and import](#installation)\n",
    "<p>\n",
    "* [Function Definition](#commonfunc)\n",
    "  * [Common functions](#commonfunc)\n",
    "  * [Q Capture Functions](#capturefunc)\n",
    "  * [Q Apply Functions](#applyfunc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Top](#top)\n",
    "<a id='intro'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupyter Notebook is sample code. No warranty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This set of Jupyter Notebook can be used to display status, performance, and health of an IBM Q Replication setup. The following Notebooks are provided:<br>\n",
    " <ul>\n",
    "  <li>Q Capture Notebook</li>\n",
    "  <li>Q Apply Notebook</li>\n",
    "  <li>Q Replication function library Jupyter Notebook (this Notebook)</li>\n",
    "</ul> \n",
    "This Notebook imports all required libraries and defines a library of Q Replication functions used by the other Q Replication Notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Top](#top)\n",
    "<a id='setup'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Individual setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customize the provided SQL files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the provided SQL queries need customization:<br>\n",
    "<b>qrep_check_qsubs_capture.sql (German version) or qrep_check_qsubs_capture_EN.sql (English version) :</b><br>\n",
    "Search for <i><u>\"change before execution\"</u></i> and adjust the query depending on the platform of your Q Capture server (<u>z/OS: <i>datacapture</i> / LUW: <i>data_capture</i>)</u><br>\n",
    "<b>qrep_check_qsubs_apply.sql (German version) or qrep_check_qsubs_apply_EN.sql (English version):</b><br>\n",
    "Search for <i><u>\"change before execution\"</u></i> and adjust the name of your <i>Q Apply user</i> (to be able to check for the required database privileges). 2x in the query.<br>\n",
    "Search for <i><u>\"change before execution\"</u></i> and adjust the query depending on the platform of your Q Apply server (<u>z/OS: <i>referencesauth</i> / LUW: <i>refauth</i></u>). 2x in the query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customize the settings to reflect your environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjust these 2 cells to your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Db2 instance home - required to be able to connect to Db2\n",
    "import sys,os,os.path\n",
    "os.environ['IBM_DB_HOME']='C:\\\\Program Files\\\\IBM\\\\SQLLIB'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the message language for the provided SQL queries:\n",
    "#   English: 'EN'\n",
    "#   German:  'DE'\n",
    "msg_lang = 'EN'\n",
    "\n",
    "# Set the path to the location where the monitoring SQL queries are stored\n",
    "qrep_asnmonitor_file_path = 'C:\\\\1Lenke\\\\1WORK\\\\DB2UDB\\\\SQL\\\\QREPL\\\\asnmonitor'\n",
    "\n",
    "# Set the path delimiter (comment either to Windows or to Linux/Unix):\n",
    "# Windows\n",
    "sql_file_delimiter = '\\\\'\n",
    "# Linux/Unix\n",
    "# sql_file_delimiter = '/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some config - don't change:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default language is EN\n",
    "if msg_lang == 'EN':\n",
    "    cust_msg_lang = '_EN'\n",
    "elif msg_lang == 'DE':\n",
    "    cust_msg_lang = ''\n",
    "else:\n",
    "    cust_msg_lang = '_EN'    \n",
    "\n",
    "# constrution of file names for the specified language\n",
    "qrep_check_qsubs_apply_file_name    = qrep_asnmonitor_file_path + sql_file_delimiter + \\\n",
    "                                      'qrep_check_qsubs_apply' + cust_msg_lang + '.sql'\n",
    "qrep_check_qsubs_capture_file_name  = qrep_asnmonitor_file_path + sql_file_delimiter + \\\n",
    "                                      'qrep_check_qsubs_capture' + cust_msg_lang + '.sql'\n",
    "qrep_monitor_apply_file_name        = qrep_asnmonitor_file_path + sql_file_delimiter + \\\n",
    "                                      'qrep_monitor_apply' + cust_msg_lang + '.sql'\n",
    "qrep_monitor_capture_file_name      = qrep_asnmonitor_file_path + sql_file_delimiter + \\\n",
    "                                      'qrep_monitor_capture' + cust_msg_lang + '.sql'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Top](#top)\n",
    "<a id='installation'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One time action to install the Db2 packages and other packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Anaconda packages missing in your installation.\n",
    "\n",
    "# !pip install ipython-sql\n",
    "\n",
    "# Install Db2 Driver\n",
    "# Uncomment the following line for Db2 11.1 and Anaconda 2\n",
    "# !pip install ibm_db\n",
    "\n",
    "# Install Db2 Driver\n",
    "# Uncomment the following line for Db2 11.1 and Anaconda 3\n",
    "# !pip install \"ibm_db==2.0.8a\"\n",
    "\n",
    "# Install SQL Alchemy\n",
    "# !pip install ibm_db_sa\n",
    "\n",
    "# Install qgrid which is required for Python Db2 Extensions (db2.iphynb)\n",
    "#!pip install qgrid\n",
    "\n",
    "# Install pixiedust which is required for Python Db2 Extensions (db2.iphynb)\n",
    "#!pip install pixiedust"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import of all required libraries <br> Execute the cell in case you restarted the Anaconda kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pixiedust database opened successfully\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div style=\"margin:10px\">\n",
       "            <a href=\"https://github.com/ibm-watson-data-lab/pixiedust\" target=\"_new\">\n",
       "                <img src=\"https://github.com/ibm-watson-data-lab/pixiedust/raw/master/docs/_static/pd_icon32.png\" style=\"float:left;margin-right:10px\"/>\n",
       "            </a>\n",
       "            <span>Pixiedust version 1.1.17</span>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import getpass\n",
    "\n",
    "import ibm_db\n",
    "import ibm_db_sa\n",
    "import sqlalchemy\n",
    "from sqlalchemy import *\n",
    "from sqlalchemy import exc\n",
    "from sqlalchemy.engine.url import URL\n",
    "%load_ext sql\n",
    "\n",
    "from datetime import date, datetime, timedelta\n",
    "\n",
    "#import datetime\n",
    "import pandas\n",
    "import ibm_db_dbi\n",
    "import json\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import sys\n",
    "import re\n",
    "import warnings\n",
    "import qgrid\n",
    "import numpy as np\n",
    "\n",
    "import pixiedust"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Top](#top)\n",
    "<a id='commonfunc'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check parameters\n",
    "def check_positive_int(varname, varvalue):\n",
    "    if not isinstance(varvalue, int):\n",
    "        print('ERROR: ' + varname + ' is not an integer')\n",
    "    elif varvalue < 0:\n",
    "         print('ERROR: ' + varname + ' is out of range (' + \n",
    "               str(varvalue) + '). Must be 0 or a positive INTEGER')\n",
    "    else:\n",
    "         print('INFO: ' + varname + ' checked ok')\n",
    "\n",
    "def check_minus_1_or_positive_int(varname, varvalue):\n",
    "    if not isinstance(varvalue, int):\n",
    "        print('ERROR: ' + varname + ' is not an integer')\n",
    "    elif varvalue < -1:\n",
    "         print('ERROR: ' + varname + ' is out of range (' + \n",
    "               str(varvalue) + '). Must be -1 or a positive INTEGER')\n",
    "    elif varvalue == 0:\n",
    "         print('ERROR: ' + varname + ' is out of range (' + \n",
    "               str(varvalue) + '). Must be -1 or a positive INTEGER')            \n",
    "    else:\n",
    "         print('INFO: ' + varname + ' checked ok')            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connection to a database, returns the connection\n",
    "def connect_to_db2(driver, user, password, host, port, alias):\n",
    "    db2_con_url = driver + '://' + user + ':' + \\\n",
    "        password + '@' + host + ':' + port + '/' + alias\n",
    "    db2_con = sqlalchemy.create_engine(db2_con_url, echo=False)\n",
    "    return db2_con"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculation of monitoring data windows, depending on the parameters \n",
    "# defined in the Q Capture or Q Apply Notebooks\n",
    "# Returns an array of 2 dates (start date and end date)\n",
    "\n",
    "def calc_mon_start_end(cust_mon_start_date, cust_mon_end_date, dft_mon_date_range):\n",
    "    \n",
    "    # convert to negative for timedelta function\n",
    "    dft_mon_date_range = dft_mon_date_range * -1\n",
    "    \n",
    "    if cust_mon_start_date == -1:\n",
    "        # tmp_date = datetime.datetime.now() +  datetime.timedelta(days = dft_mon_date_range)\n",
    "        tmp_date = datetime.now() + timedelta(days = dft_mon_date_range)\n",
    "        calc_mon_start_date = (tmp_date.strftime(\"%Y-%m-%d-%H.%M.%S.%f\"))\n",
    "    else:\n",
    "        calc_mon_start_date = cust_mon_start_date\n",
    "\n",
    "    if cust_mon_end_date == -1:\n",
    "        tmp_date = datetime.now() \n",
    "        calc_mon_end_date = (tmp_date.strftime(\"%Y-%m-%d-%H.%M.%S.%f\"))\n",
    "    else:\n",
    "        calc_mon_end_date = cust_mon_end_date\n",
    "\n",
    "    return [calc_mon_start_date,calc_mon_end_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the default colour coding for the SEVerity column. \n",
    "# Background colour and font colour \n",
    "\n",
    "def sev_background(value):\n",
    "  \"\"\"\n",
    "  Background color in a dateframe:\n",
    "  red for \"ERROR\", yellow for \"WARNING\",\n",
    "  green for \"INFO\". Does not color NaN\n",
    "  values.\n",
    "  \"\"\"\n",
    "\n",
    "  if value == 'ERROR':\n",
    "    background = 'red'\n",
    "  elif value == 'WARNING':\n",
    "    background = 'yellow'\n",
    "  else:\n",
    "    background = 'green'\n",
    "    \n",
    "  return 'background: %s' % background\n",
    "\n",
    "def sev_foreground(value):\n",
    "  \"\"\"\n",
    "  Font color in a dateframe:\n",
    "  white for \"ERROR\", black for \"WARNING\",\n",
    "  white for \"INFO\". Does not color NaN\n",
    "  values.\n",
    "  \"\"\"\n",
    "\n",
    "  if value == 'ERROR':\n",
    "    color = 'white'\n",
    "  elif value == 'INFO':\n",
    "    color = 'white'\n",
    "  else:\n",
    "    color = 'black'\n",
    "    \n",
    "  return 'color: %s' % color"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Top](#top)\n",
    "<a id='capturefunc'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q Capture functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q Capture Status Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read the query for the Q Capture status monitor from \n",
    "# file (qrep_monitor_capture.sql) and to execute the query. \n",
    "# Returns a data frame\n",
    "\n",
    "def get_capture_status(conn):\n",
    "\n",
    "    if not os.path.isfile(qrep_monitor_capture_file_name):\n",
    "        print('File does not exist.')\n",
    "    else:\n",
    "        with open(qrep_monitor_capture_file_name , 'r') as f:\n",
    "            sql_text_list = f.read()\n",
    "\n",
    "    sql_text = ''.join(sql_text_list)        \n",
    "\n",
    "    f.close()\n",
    "\n",
    "    # debug\n",
    "    # print(sql_text)\n",
    "    \n",
    "    try:\n",
    "        df = pandas.read_sql_query(sql_text, conn)\n",
    "    except exc.SQLAlchemyError as ex:\n",
    "        print('We got a problem......:' + str(ex))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read data from IBMQREP_SENDQUEUES \n",
    "# Returns a data frame\n",
    "\n",
    "def get_sendq_state(conn):\n",
    "    sql_text = \"select 'Queues' as QUEUES, A.ACTIVE, I.INACTIVE from ( \\\n",
    "    (select 'NUM_QUEUES' as X, count(*) as active from ibmqrep_sendqueues where state = 'A')  A \\\n",
    "    inner join  \\\n",
    "    (select 'NUM_QUEUES' as X, count(*) as inactive from ibmqrep_sendqueues where state = 'I')  I \\\n",
    "    ON A.X = I.X );\"\n",
    "    \n",
    "    try:\n",
    "        df = pandas.read_sql_query(sql_text, conn)\n",
    "    except exc.SQLAlchemyError as ex:\n",
    "        print('We got a problem......:' + str(ex))\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read data from IBMQREP_SUBS\n",
    "# Returns a data frame\n",
    "\n",
    "def get_substate_by_sendq(conn):\n",
    "    sql_text = \"select A.SENDQ as SENDQ, coalesce(A.ACTIVE , 0) as ACTIVE, coalesce(I.INACTIVE , 0) as INACTIVE \\\n",
    "    from ( \\\n",
    "    (select SENDQ, count(*) as active from ibmqrep_subs where state = 'A' group by sendq)  A  \\\n",
    "    full outer join   \\\n",
    "    (select SENDQ, count(*) as inactive from ibmqrep_subs where state = 'I' group by sendq)  I  \\\n",
    "    ON A.SENDQ = I.SENDQ \\\n",
    "    );\"\n",
    "    \n",
    "    try:\n",
    "        df = pandas.read_sql_query(sql_text, conn)\n",
    "    except exc.SQLAlchemyError as ex:\n",
    "        print('We got a problem......:' + str(ex))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q Capture Log Reader Performance Statistics - CAPMON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read data from IBMQREP_CAPMON\n",
    "# Returns a data frame\n",
    "\n",
    "def get_perf_logrd(conn, range):\n",
    "\n",
    "    # Default: testschema = ''  to read from default schema\n",
    "    # Set it to a specific test schema only for test purposes\n",
    "    testschema = ''\n",
    "    # testschema = 'DB2R_20190509.'\n",
    "\n",
    "    if range == -1:\n",
    "        filterclause = \"\"\n",
    "    else:\n",
    "        filterclause = \"where monitor_time > (select max(monitor_time) - \" + str(range) + \" days \\\n",
    "                        from \" + testschema + \"ibmqrep_capmon ) \"\n",
    "        \n",
    "    sql_text = \"select varchar(date(monitor_time)) concat '-' \\\n",
    "            concat varchar(hour(monitor_time)) concat ':00' as monitor_date, \\\n",
    "            sum(rows_processed) as sum_rows_processed, \\\n",
    "            dec(max(current_memory)) / 1024 / 1024 as max_memory_mb, \\\n",
    "            sum(trans_spilled) as sum_trans_spilled \\\n",
    "            from \" + testschema + \"ibmqrep_capmon \" + filterclause + \\\n",
    "            \" group by date(monitor_time), hour(monitor_time) \\\n",
    "            order by date(monitor_time), hour(monitor_time)\"\n",
    "    \n",
    "    try:\n",
    "        df = pandas.read_sql_query(sql_text, conn)\n",
    "    except exc.SQLAlchemyError as ex:\n",
    "        print('We got a problem......:' + str(ex))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a figure to display the following values of IBMQREP_CAPMON:\n",
    "#  - CURRENT_MEMORY\n",
    "#  - ROWS_PROCESSED\n",
    "#  - TRANS_SPILLED\n",
    "# Returns the plot\n",
    "\n",
    "def plot_capmon(df_logr,numdays):\n",
    "\n",
    "    fig = plt.figure(figsize=(20,10))\n",
    "    \n",
    "    if numdays == -1:\n",
    "        limittext = ' - All available data'\n",
    "    else:\n",
    "        limittext = ' - Last ' + str(numdays) + ' days of CAPMON data'\n",
    "\n",
    "    # First axes: sum_rows_processed\n",
    "    ax1 = fig.add_subplot(111,title='Capture log reader throughput and memory per hour')\n",
    "\n",
    "    # the ax keyword sets the axis that the data frame plots to\n",
    "    df_logr.plot(ax=ax1, x='monitor_date', y='sum_rows_processed',\n",
    "                 kind='line', linewidth=1, linestyle='-', color='m', \n",
    "                 title='CAPMON statistics ' + limittext)\n",
    "    \n",
    "    plt.ticklabel_format(style='plain', axis='y')\n",
    "    plt.xticks(df_logr.index,df_logr['monitor_date'].values,rotation=70)\n",
    "\n",
    "    xticks1 = plt.gca().xaxis.get_major_ticks()\n",
    "\n",
    "    for j in range(len(xticks1)):\n",
    "        if (numdays > 5) or (numdays == -1):\n",
    "            if j % 24 != 0:\n",
    "                xticks1[j].set_visible(False)\n",
    "            else:\n",
    "                xticks1[j].set_visible(True)\n",
    "        elif numdays > 2:                    \n",
    "            if j % 4 != 0:\n",
    "                xticks1[j].set_visible(False)\n",
    "            else:\n",
    "                xticks1[j].set_visible(True)\n",
    "        else:\n",
    "            if j % 2 != 0:\n",
    "                xticks1[j].set_visible(False)\n",
    "            else:\n",
    "                xticks1[j].set_visible(True)                \n",
    "\n",
    "    ax1.set_xlabel('monitor date (yyyy-mm-dd-hh:mm)')\n",
    "    ax1.set_ylabel('rows processed per hour')\n",
    "\n",
    "    # Second axes: max_memory_mb    \n",
    "    ax2 = ax1.twinx()\n",
    "\n",
    "    df_logr.plot(ax=ax2, x='monitor_date', y='max_memory_mb', \n",
    "                kind='line', linewidth=1, linestyle=':', color='b')\n",
    "\n",
    "    ax2.set_ylabel('max memory (MB)')\n",
    "    plt.legend(loc='upper left')\n",
    "\n",
    "    # Third axes: trans_spilled\n",
    "    ax3 = ax1.twinx()\n",
    "    ax3.spines[\"right\"].set_position((\"axes\", 1.1))\n",
    "\n",
    "    if max(df_logr['sum_trans_spilled']) == 0: \n",
    "        maxtsp = 5\n",
    "    else:\n",
    "        maxtsp = max(df_logr['sum_trans_spilled'])\n",
    "\n",
    "    ax3.set_ylim(0, maxtsp)\n",
    "\n",
    "    df_logr.plot(ax=ax3, x='monitor_date', y='sum_trans_spilled', \n",
    "                kind='bar', legend=False)\n",
    "\n",
    "    ax3.set_ylabel('monster trans')\n",
    "    \n",
    "    return(plt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q Capture Publisher Performance Statistics - CAPQMON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read data from IBMQREP_CAPQMON\n",
    "# Returns a data frame\n",
    "    \n",
    "def get_perf_publ(conn, range):\n",
    "    \n",
    "    # Default: testschema = ''  to read from default schema\n",
    "    # Set it to a specific test schema only for test purposes\n",
    "    testschema = ''\n",
    "    # testschema = 'DB2R_20190509.'\n",
    "\n",
    "    if range == -1:\n",
    "        filterclause = \"\"\n",
    "    else:\n",
    "        filterclause = \"where monitor_time > (select max(monitor_time) - \" + str(range) + \" days \\\n",
    "                        from \" + testschema + \"ibmqrep_capmon ) \"  \n",
    "    \n",
    "    sql_text = \"select varchar(date(monitor_time)) concat '-' \\\n",
    "            concat varchar(hour(monitor_time)) concat ':00' as monitor_date, \\\n",
    "            sendq, \\\n",
    "            sum(rows_published) as sum_rows_published, \\\n",
    "            sum(trans_published) as sum_trans_published, \\\n",
    "            sum(mq_messages) as sum_mq_messages, \\\n",
    "            max(xmitqdepth) as max_xmitqdepth \\\n",
    "            from \" + testschema + \"ibmqrep_capqmon \" + filterclause + \\\n",
    "            \" group by sendq, date(monitor_time), hour(monitor_time) \\\n",
    "            order by date(monitor_time), hour(monitor_time)\"\n",
    "\n",
    "    try:\n",
    "        df = pandas.read_sql_query(sql_text, conn)\n",
    "    except exc.SQLAlchemyError as ex:\n",
    "        print('We got a problem......:' + str(ex))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todo: xticks only for final plot. Legend misplaced.\n",
    "\n",
    "# Function to create a figure with n plots (one lot per receive queue) to display the \n",
    "# following values of IBMQREP_CAPQMON:\n",
    "#  - SUM_ROWS_PUBLISHED\n",
    "#  - SUM_MQ_MESSAGES\n",
    "#  - XMITQDEPTH\n",
    "# Returns the plots\n",
    "\n",
    "def plot_capqmon(df_publ_allq,qs,numdays):\n",
    "\n",
    "    # The hight of the following figures depends on the number of distinct send queues.\n",
    "    # The more queues, the less space per individual queue (to limit the size of the figure)\n",
    "    if numqs >=4:\n",
    "        calc_fig_height = numqs * 6\n",
    "    if numqs >=2:\n",
    "        calc_fig_height = numqs * 8\n",
    "    else:\n",
    "        calc_fig_height = numqs * 10\n",
    "    \n",
    "    fig3 = plt.figure(1,figsize=(20,calc_fig_height))\n",
    "\n",
    "    if numdays == -1:\n",
    "        limittext = ' - All available data'\n",
    "    else:\n",
    "        limittext = ' - Last ' + str(numdays) + ' days of CAPQMON data'\n",
    "        \n",
    "    # get a 2-dimensional object\n",
    "    ax = {}\n",
    "    \n",
    "    # print('DEBUG: qs=' + str(qs))\n",
    "    \n",
    "    # Loop, for each reqeive queue\n",
    "    for i in range(0,qs):\n",
    "        # print('DEBUG: i=' + str(i) + '; ' + df_publ_allq.sendq.unique()[i])\n",
    "\n",
    "        # First axes: sum_rows_published,sum_mq_messages\n",
    "\n",
    "        # Axis position\n",
    "        #  First parameter: Number of plots = number of queues\n",
    "        #  Second parameter: Horizintal position or column (always 1 = only 1 column)\n",
    "        #  Third parameter: Vertical position or row (1, 2, 3, ... depending on the loop)\n",
    "        ax[i,0] = fig3.add_subplot(qs,1,i+1)\n",
    "\n",
    "        # Plot. Data frame limited to one queue per iteration\n",
    "        df_publ_allq[df_publ_allq.sendq == df_publ_allq.sendq.unique()[i]].plot(ax=ax[i,0], \n",
    "            x='monitor_date', \n",
    "            y=['sum_rows_published','sum_mq_messages'],\n",
    "            kind='line', linewidth=1, linestyle='-', \n",
    "            title=df_publ_allq.sendq.unique()[i] + ' APPLYMON statistics' + limittext)\n",
    "        \n",
    "        # monitor_date as xticks\n",
    "        plt.xticks(df_publ_allq.index,df_publ_allq['monitor_date'].values,rotation=70)\n",
    "\n",
    "        xticks1 = plt.gca().xaxis.get_major_ticks()\n",
    "\n",
    "        # setting the tick density        \n",
    "        for j in range(len(xticks1)):\n",
    "            if (numdays > 5) or (numdays == -1):\n",
    "                if j % 24 != 0:\n",
    "                    xticks1[j].set_visible(False)\n",
    "                else:\n",
    "                    xticks1[j].set_visible(True)\n",
    "            elif numdays > 2:                    \n",
    "                if j % 4 != 0:\n",
    "                    xticks1[j].set_visible(False)\n",
    "                else:\n",
    "                    xticks1[j].set_visible(True)\n",
    "            else:\n",
    "                if j % 2 != 0:\n",
    "                    xticks1[j].set_visible(False)\n",
    "                else:\n",
    "                    xticks1[j].set_visible(True)                \n",
    "    \n",
    "        ax[i,0].set_xlabel('monitor date (yyyy-mm-dd-hh:mm)')\n",
    "        ax[i,0].set_ylabel('Number of')\n",
    "\n",
    "        # Second axes: Rows processed\n",
    "        \n",
    "        ax[i,1] = ax[i,0].twinx()\n",
    "\n",
    "        # Plot. Data frame limited to one queue per iteration        \n",
    "        df_publ_allq[df_publ_allq.sendq == df_publ_allq.sendq.unique()[i]].plot(ax=ax[i,1], \n",
    "            x='monitor_date', \n",
    "            y='max_xmitqdepth', \n",
    "            kind='line', linewidth=1, linestyle='-', color='m')\n",
    "\n",
    "        plt.ticklabel_format(style='plain', axis='y')\n",
    "        ax[i,1].set_ylabel('Max XMIT Queue Depth')\n",
    "        plt.legend(loc='upper left')\n",
    "\n",
    "    return(plt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q Capture Queue and Sub Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read data from IBMQREP_SENDQUEUES\n",
    "# Returns a data frame\n",
    "# The column list is not hard coded but retrieved from the catalog (all columns) to more flexibly\n",
    "# react on new control table columns\n",
    "\n",
    "def get_sendq_details(conn, schema):\n",
    "    cat_stmt = \"select name from sysibm.syscolumns \\\n",
    "                where tbcreator = '\" + schema + \"' \\\n",
    "                  and tbname = 'IBMQREP_SENDQUEUES' \\\n",
    "                order by COLNO;\"\n",
    "    \n",
    "    try:\n",
    "        cat_df = pandas.read_sql_query(cat_stmt, conn)\n",
    "    except exc.SQLAlchemyError as ex:\n",
    "        print('We got a problem......:' + str(ex))\n",
    "    \n",
    "    for index, row in cat_df.iterrows():\n",
    "        if index == 0:\n",
    "            collist = row['name']\n",
    "        else:\n",
    "            collist = collist + ', ' + row['name']\n",
    "        \n",
    "    sql_text = \"SELECT \" + collist + \" FROM IBMQREP_SENDQUEUES order by sendq;\"\n",
    "    \n",
    "    try:\n",
    "        df = pandas.read_sql_query(sql_text, conn)\n",
    "    except exc.SQLAlchemyError as ex:\n",
    "        print('We got a problem......:' + str(ex))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read data from IBMQREP_SUBS\n",
    "# Returns a data frame\n",
    "# The column list is not hard coded but retrieved from the catalog (all columns) to more flexibly\n",
    "# react on new control table columns\n",
    "\n",
    "def get_subs_details(conn, schema):\n",
    "    cat_stmt = \"select name from sysibm.syscolumns \\\n",
    "                where tbcreator = '\" + schema + \"' \\\n",
    "                  and tbname = 'IBMQREP_SUBS' \\\n",
    "                order by COLNO;\"\n",
    "    \n",
    "    try:\n",
    "        cat_df = pandas.read_sql_query(cat_stmt, conn)\n",
    "    except exc.SQLAlchemyError as ex:\n",
    "        print('We got a problem......:' + str(ex))\n",
    "    \n",
    "    for index, row in cat_df.iterrows():\n",
    "        if index == 0:\n",
    "            collist = row['name']\n",
    "        else:\n",
    "            collist = collist + ', ' + row['name']\n",
    "            \n",
    "    sql_text = \"SELECT \" + collist + \" FROM IBMQREP_SUBS order by subname;\"\n",
    "    \n",
    "    try:\n",
    "        df = pandas.read_sql_query(sql_text, conn)\n",
    "    except exc.SQLAlchemyError as ex:\n",
    "        print('We got a problem......:' + str(ex))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q Capture Anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read the query for the Q Subscription check from \n",
    "# file (qrep_check_subs_capture.sql) and to execute the query. \n",
    "# Returns a data frame\n",
    "\n",
    "def get_capture_anomylies(conn):\n",
    "\n",
    "    if not os.path.isfile(qrep_check_qsubs_capture_file_name):\n",
    "        print('File does not exist.')\n",
    "    else:\n",
    "        with open(qrep_check_qsubs_capture_file_name , 'r') as f:\n",
    "            sql_text_list = f.read()\n",
    "\n",
    "    sql_text = ''.join(sql_text_list)        \n",
    "\n",
    "    f.close()\n",
    "\n",
    "    # debug\n",
    "    # print(sql_text)\n",
    "    \n",
    "    try:\n",
    "        df = pandas.read_sql_query(sql_text, conn)\n",
    "    except exc.SQLAlchemyError as ex:\n",
    "        print('We got a problem......:' + str(ex))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q Capture Control Table Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read data from IBMQREP_CAPPARMS\n",
    "# Returns a data frame\n",
    "# The column list is not hard coded but retrieved from the catalog (all columns) to more flexibly\n",
    "# react on new control table columns\n",
    "\n",
    "def get_capparms(conn, schema):\n",
    "    cat_stmt = \"select name from sysibm.syscolumns \\\n",
    "                where tbcreator = '\" + schema + \"' \\\n",
    "                  and tbname = 'IBMQREP_CAPPARMS' \\\n",
    "                order by COLNO;\"\n",
    "    \n",
    "    try:\n",
    "        cat_df = pandas.read_sql_query(cat_stmt, conn)\n",
    "    except exc.SQLAlchemyError as ex:\n",
    "        print('We got a problem......:' + str(ex))\n",
    "    \n",
    "    for index, row in cat_df.iterrows():\n",
    "        if index == 0:\n",
    "            collist = row['name']\n",
    "        else:\n",
    "            collist = collist + ', ' + row['name']\n",
    "            \n",
    "    sql_text = \"SELECT \" + collist + \" FROM IBMQREP_CAPPARMS;\"\n",
    "    \n",
    "    try:\n",
    "        df = pandas.read_sql_query(sql_text, conn)\n",
    "    except exc.SQLAlchemyError as ex:\n",
    "        print('We got a problem......:' + str(ex))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read data from IBMQREP_CAPTRACE (only numrows)\n",
    "# Returns a data frame\n",
    "# The column list is not hard coded but retrieved from the catalog (all columns) to more flexibly\n",
    "# react on new control table columns\n",
    "\n",
    "def get_captrace(conn, schema, numrows):\n",
    "    cat_stmt = \"select name from sysibm.syscolumns \\\n",
    "                where tbcreator = '\" + schema + \"' \\\n",
    "                  and tbname = 'IBMQREP_CAPTRACE' \\\n",
    "                order by COLNO;\"\n",
    "    \n",
    "    try:\n",
    "        cat_df = pandas.read_sql_query(cat_stmt, conn)\n",
    "    except exc.SQLAlchemyError as ex:\n",
    "        print('We got a problem......:' + str(ex))\n",
    "    \n",
    "    for index, row in cat_df.iterrows():\n",
    "        if index == 0:\n",
    "            collist = row['name']\n",
    "        else:\n",
    "            collist = collist + ', ' + row['name']\n",
    "            \n",
    "    sql_text = \"SELECT \" + collist + \" FROM ibmqrep_captrace order by trace_time desc \"\n",
    "\n",
    "    if numrows > 0:\n",
    "        sql_text = sql_text + \" fetch first \" + str(numrows) + \" rows only;\"\n",
    "    else:\n",
    "        sql_text = sql_text + \";\"\n",
    "        \n",
    "    try:\n",
    "        df = pandas.read_sql_query(sql_text, conn)\n",
    "    except exc.SQLAlchemyError as ex:\n",
    "        print('We got a problem......:' + str(ex))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read data from IBMQREP_CAPMON with a WHERE clause respecting the \n",
    "# calculated data range. Returns a data frame\n",
    "# The column list is not hard coded but retrieved from the catalog (all columns) to more flexibly\n",
    "# react on new control table columns\n",
    "\n",
    "def get_capmon(conn, schema, start_date, end_date):\n",
    "    cat_stmt = \"select name from sysibm.syscolumns \\\n",
    "                where tbcreator = '\" + schema + \"' \\\n",
    "                  and tbname = 'IBMQREP_CAPMON' \\\n",
    "                order by COLNO;\"\n",
    "    \n",
    "    try:\n",
    "        cat_df = pandas.read_sql_query(cat_stmt, conn)\n",
    "    except exc.SQLAlchemyError as ex:\n",
    "        print('We got a problem......:' + str(ex))\n",
    "\n",
    "    listOfHexCols = ['RESTART_SEQ' , 'CURRENT_SEQ', 'RESTART_MAXCMTSEQ']\n",
    "    for index, row in cat_df.iterrows():\n",
    "        if row['name'] in listOfHexCols:\n",
    "            colexp = \"hex(\" + row['name'] + \") as \" + row['name']\n",
    "        else: \n",
    "            colexp = row['name']\n",
    "\n",
    "        if index == 0:\n",
    "            collist = colexp\n",
    "        else:\n",
    "            collist = collist + ', ' + colexp\n",
    "            \n",
    "    sql_text = \"SELECT \" + collist + \" FROM IBMQREP_CAPMON \\\n",
    "                where monitor_time >= '\" + start_date + \"' \\\n",
    "                  and  monitor_time <= '\" + end_date + \"' \\\n",
    "                order by monitor_time desc;\"    \n",
    "    \n",
    "    try:\n",
    "        df = pandas.read_sql_query(sql_text, conn)\n",
    "    except exc.SQLAlchemyError as ex:\n",
    "        print('We got a problem......:' + str(ex))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read data from IBMQREP_CAPQMON with a WHERE clause respecting the \n",
    "# calculated data range. Returns a data frame\n",
    "# The column list is not hard coded but retrieved from the catalog (all columns) to more flexibly\n",
    "# react on new control table columns\n",
    "\n",
    "def get_capqmon(conn, schema, start_date, end_date):\n",
    "    cat_stmt = \"select name from sysibm.syscolumns \\\n",
    "                where tbcreator = '\" + schema + \"' \\\n",
    "                  and tbname = 'IBMQREP_CAPQMON' \\\n",
    "                order by COLNO;\"\n",
    "    \n",
    "    try:\n",
    "        cat_df = pandas.read_sql_query(cat_stmt, conn)\n",
    "    except exc.SQLAlchemyError as ex:\n",
    "        print('We got a problem......:' + str(ex))\n",
    "\n",
    "    listOfHexCols = ['CURRENT_SEQ' , 'RESTART_SEQ', 'RESTART_MAXCMTSEQ']\n",
    "    for index, row in cat_df.iterrows():\n",
    "        if row['name'] in listOfHexCols:\n",
    "            colexp = \"hex(\" + row['name'] + \") as \" + row['name']\n",
    "        else: \n",
    "            colexp = row['name']\n",
    "\n",
    "        if index == 0:\n",
    "            collist = colexp\n",
    "        else:\n",
    "            collist = collist + ', ' + colexp\n",
    "            \n",
    "    sql_text = \"SELECT \" + collist + \" FROM IBMQREP_CAPQMON \\\n",
    "                where monitor_time >= '\" + start_date + \"' \\\n",
    "                  and  monitor_time <= '\" + end_date + \"' \\\n",
    "                order by monitor_time desc;\"   \n",
    "    \n",
    "    try:\n",
    "        df = pandas.read_sql_query(sql_text, conn)\n",
    "    except exc.SQLAlchemyError as ex:\n",
    "        print('We got a problem......:' + str(ex))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Top](#top)\n",
    "<a id='applyfunc'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q Apply functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q Apply status overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read the query for the Q Apply status monitor from \n",
    "# file (qrep_monitor_apply.sql) and to execute the query. \n",
    "# Returns a data frame\n",
    "\n",
    "def get_apply_status(conn):\n",
    "\n",
    "    if not os.path.isfile(qrep_monitor_apply_file_name):\n",
    "        print('File does not exist.')\n",
    "    else:\n",
    "        with open(qrep_monitor_apply_file_name , 'r') as f:\n",
    "            sql_text_list = f.read()\n",
    "\n",
    "    sql_text = ''.join(sql_text_list)        \n",
    "\n",
    "    f.close()\n",
    "\n",
    "    # debug\n",
    "    # print(sql_text)\n",
    "    \n",
    "    try:\n",
    "        df = pandas.read_sql_query(sql_text, conn)\n",
    "    except exc.SQLAlchemyError as ex:\n",
    "        print('We got a problem......:' + str(ex))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read data from IBMQREP_RECVQUEUES \n",
    "# Returns a data frame\n",
    "\n",
    "def get_recvq_state(conn):\n",
    "    sql_text = \"select 'Queues' as QUEUES, A.ACTIVE, I.INACTIVE from ( \\\n",
    "    (select 'NUM_QUEUES' as X, count(*) as active from ibmqrep_recvqueues where state = 'A')  A \\\n",
    "    inner join  \\\n",
    "    (select 'NUM_QUEUES' as X, count(*) as inactive from ibmqrep_recvqueues where state = 'I')  I \\\n",
    "    ON A.X = I.X );\"\n",
    "    \n",
    "    try:\n",
    "        df = pandas.read_sql_query(sql_text, conn)\n",
    "    except exc.SQLAlchemyError as ex:\n",
    "        print('We got a problem......:' + str(ex))\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read data from IBMQREP_TARGETS\n",
    "# Returns a data frame\n",
    "\n",
    "def get_substate_by_recvq(conn):\n",
    "    sql_text = \"select A.RECVQ as RECVQ, coalesce(A.ACTIVE , 0) as ACTIVE, coalesce(I.INACTIVE , 0) as INACTIVE \\\n",
    "    from ( \\\n",
    "    (select RECVQ, count(*) as active from ibmqrep_targets where state = 'A' group by recvq)  A  \\\n",
    "    full outer join   \\\n",
    "    (select RECVQ, count(*) as inactive from ibmqrep_targets where state = 'I' group by recvq)  I  \\\n",
    "    ON A.RECVQ = I.RECVQ \\\n",
    "    );\"\n",
    "    \n",
    "    try:\n",
    "        df = pandas.read_sql_query(sql_text, conn)\n",
    "    except exc.SQLAlchemyError as ex:\n",
    "        print('We got a problem......:' + str(ex))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q Apply Subscription type by RECVQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read different subscription types from IBMQREP_TARGETS\n",
    "# Returns a data frame\n",
    "\n",
    "def get_subtype_by_recvq(conn):\n",
    "    sql_text = \"select recvq, \\\n",
    "         case subtype \\\n",
    "           when 'B' then 'BIDI' \\\n",
    "           when 'P' then 'P2P' \\\n",
    "           when 'U' then \\\n",
    "             case target_type \\\n",
    "               when 1 then 'USER TABLE' \\\n",
    "               when 5 then 'STOREDPROC' \\\n",
    "               when 2 then  \\\n",
    "                 case \\\n",
    "                   when CCD_CONDENSED = 'Y' and CCD_COMPLETE = 'Y' then \\\n",
    "                     'COND/COMP CCD' \\\n",
    "                   when CCD_CONDENSED = 'N' and CCD_COMPLETE = 'N' then \\\n",
    "                     'NON-COND/NON-COMP CCD' \\\n",
    "                   when CCD_CONDENSED = 'Y' and CCD_COMPLETE = 'N' then \\\n",
    "                     'COND/NON-COMP CCD' \\\n",
    "                   when CCD_CONDENSED = 'N' and CCD_COMPLETE = 'Y' then \\\n",
    "                     'NON-COND/COMP CCD' \\\n",
    "                   else 'OTHER CCD' \\\n",
    "                 end \\\n",
    "               else 'OTHER TARGET TYPE' \\\n",
    "             end \\\n",
    "           else 'OTHER SUBTYPE' \\\n",
    "         end as calc_type, \\\n",
    "         count(*) as count_type  \\\n",
    "         from lsn.ibmqrep_targets  \\\n",
    "         group by recvq, subtype, target_type, ccd_condensed, ccd_complete;\"\n",
    "    \n",
    "    try:\n",
    "        df = pandas.read_sql_query(sql_text, conn)\n",
    "    except exc.SQLAlchemyError as ex:\n",
    "        print('We got a problem......:' + str(ex))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a figure with n donut plots (one lot per receive queue) to display the \n",
    "# subscription types by receive queue:\n",
    "\n",
    "def plot_types(df_st,numqs,figrows):\n",
    "\n",
    "    # The hight of the following figures depends on the number of distinct receive queues.\n",
    "    # The more queues, the less space per individual queue (to limit the size of the figure)\n",
    "    \n",
    "    if numqs > 10:\n",
    "        calc_fig_height = numqs * 2\n",
    "    elif numqs > 2:\n",
    "        calc_fig_height = numqs * 4\n",
    "    else:\n",
    "        calc_fig_height = numqs * 10\n",
    "    \n",
    "    # rows = number of receive queues / 2; columns always 2\n",
    "    figpie, axs = plt.subplots(figrows,2)\n",
    "\n",
    "    figpie.set_figheight(calc_fig_height)\n",
    "    figpie.set_figwidth(20)\n",
    "\n",
    "    # create a white circle to later convert the pie into a donut\n",
    "    n, radii = 50, [0, .5]\n",
    "    theta = np.linspace(0, 2*np.pi, n, endpoint=True)\n",
    "    xs = np.outer(radii, np.cos(theta))\n",
    "    ys = np.outer(radii, np.sin(theta))\n",
    "    xs[1,:] = xs[1,::-1]\n",
    "    ys[1,:] = ys[1,::-1]\n",
    "\n",
    "    # Loop, plot a distinct pie for each receive queue \n",
    "    for i in range(0,numqs):\n",
    "        \n",
    "        # reset the colors list for each queue. The colors have to be set for each queue\n",
    "        # individually, to be able to assign the same color to always the same subscription \n",
    "        # type. Example: bidi always blue\n",
    "        mycolors = []\n",
    "       \n",
    "        # current row = current queue devided by 2 as int - example int(5 / 2) = int(2.5) = 2\n",
    "        row = int(i / 2)\n",
    "        # current column = current queue modulo 2 - example: 5 % 2 = 1\n",
    "        col = i % 2\n",
    "        \n",
    "        # store the name of the current recvq\n",
    "        eval_recvq = df_st.recvq.unique()[i]\n",
    "\n",
    "        # Filter, only current RECVQ\n",
    "        df_st_pl = df_st[df_st.recvq == eval_recvq]\n",
    "    \n",
    "        # All distinct subscription types for the current recvq\n",
    "        qtypes = df_st_pl.calc_type.unique()\n",
    "    \n",
    "        # Loop through all types of that queue and assign consistent colors,\n",
    "        # same color for same type in all pies\n",
    "        for c in range(0,len(qtypes)):\n",
    "\n",
    "            # print(mycolors)\n",
    "            # print(str(c), qtypes[c])\n",
    "            if qtypes[c] == 'BIDI':\n",
    "                mycolors.append('tab:blue')\n",
    "            elif qtypes[c] == 'P2P': \n",
    "                mycolors.append('tab:cyan')\n",
    "            elif qtypes[c] == 'USER TABLE': \n",
    "                mycolors.append('tab:green')\n",
    "            elif qtypes[c] == 'STOREDPROC': \n",
    "                mycolors.append('tab:red')\n",
    "            elif qtypes[c] == 'COND/COMP CCD': \n",
    "                mycolors.append('tab:pink')\n",
    "            elif qtypes[c] == 'NON-COND/NON-COMP CCD': \n",
    "                mycolors.append('tab:orange')\n",
    "            elif qtypes[c] == 'COND/NON-COMP CCD': \n",
    "                mycolors.append('tab:olive')\n",
    "            elif qtypes[c] == 'NON-COND/COMP CCD': \n",
    "                mycolors.append('tab:grey')\n",
    "            elif qtypes[c] == 'OTHER CCD': \n",
    "                mycolors.append('orangered')\n",
    "            elif qtypes[c] == 'OTHER TARGET TYPE': \n",
    "                mycolors.append('orangered')\n",
    "            else: \n",
    "                mycolors.append('orangered')\n",
    "            # print(str(c), qtypes[c], mycolors[c])\n",
    "            \n",
    "        # get a total of all subs across all types. Required to convert relative shares to absolute shares\n",
    "        total = sum(df_st_pl['count_type'])\n",
    "    \n",
    "        axs[row,col].axis('equal') \n",
    "    \n",
    "        # plot pie, with absolute values\n",
    "        axs[row,col].pie(df_st_pl['count_type'], labels=df_st_pl['calc_type'], colors=mycolors, \n",
    "                         autopct=lambda p: '{:.0f}'.format(p * total / 100), pctdistance=0.7, startangle=90, \n",
    "                         textprops={'fontsize': 14, 'color' : 'black'})\n",
    "    \n",
    "        # convert pie into a donut \n",
    "        axs[row,col].fill(np.ravel(xs), np.ravel(ys), edgecolor='white', color='white')\n",
    "        axs[row,col].set_title(eval_recvq, fontdict={'fontsize' : 16, 'fontweight' : 'bold'}) \n",
    "        \n",
    "    return(plt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q Apply Performance Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read data from IBMQREP_APPLYMON, GROUPed BY HOUR(MONITOR_DATE)\n",
    "# Returns a data frame\n",
    "\n",
    "def get_perf_applymon(conn, range):\n",
    "\n",
    "    # Default: testschema = ''  to read from default schema\n",
    "    # Set it to a specific test schema only for test purposes\n",
    "    testschema = ''\n",
    "    # testschema = 'IDH1I1A_20190509.'\n",
    "    \n",
    "    if range == -1:\n",
    "        filterclause = \"\"\n",
    "    else:\n",
    "        filterclause = \"where monitor_time > (select max(monitor_time) - \" + str(range) + \" days \\\n",
    "                        from \" + testschema + \"ibmqrep_applymon ) \"\n",
    "            \n",
    "    sql_text = \"select varchar(date(monitor_time)) concat '-' \\\n",
    "            concat right(digits(hour(monitor_time)) , 2) concat ':00' as monitor_date, \\\n",
    "            recvq, \\\n",
    "            dec(max(capture_latency)) / 1000 as max_capture_latency_sec, \\\n",
    "            dec(max(qlatency)) / 1000 as max_qlatency_sec, \\\n",
    "            dec(max(apply_latency)) / 1000 as max_apply_latency_sec, \\\n",
    "            sum(rows_processed) as sum_rows_processed, \\\n",
    "            max(qdepth) as max_qdepth \\\n",
    "            from \" + testschema + \"ibmqrep_applymon \" + filterclause + \\\n",
    "            \" group by recvq, date(monitor_time), hour(monitor_time) \\\n",
    "            order by date(monitor_time), hour(monitor_time), recvq;\"\n",
    "   \n",
    "    try:\n",
    "        df = pandas.read_sql_query(sql_text, conn)\n",
    "    except exc.SQLAlchemyError as ex:\n",
    "        print('We got a problem......:' + str(ex)) \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todo: xticks only for final plot. Legend misplaced.\n",
    "\n",
    "# Function to create a figure with n plots (one lot per receive queue) to display the \n",
    "# following values of IBMQREP_APPLYMON:\n",
    "#  - CAPTURE_LATENCY\n",
    "#  - APPLY_LATENCY\n",
    "#  - QLATENCY\n",
    "#  - ROWS_PROCESSED\n",
    "# Returns the plots\n",
    "\n",
    "def plot_lat(lat_df_allq,qs,numdays,grouping):\n",
    "\n",
    "    # The hight of the following figures depends on the number of distinct receive queues.\n",
    "    # The more queues, the less space per individual queue (to limit the size of the figure)\n",
    "    if numqs >=4:\n",
    "        calc_fig_height = numqs * 6\n",
    "    if numqs >=2:\n",
    "        calc_fig_height = numqs * 8\n",
    "    else:\n",
    "        calc_fig_height = numqs * 10\n",
    "    \n",
    "    fig3 = plt.figure(1,figsize=(20,calc_fig_height))\n",
    "\n",
    "    if numdays == -1:\n",
    "        limittext = ' - All available data'\n",
    "    else:\n",
    "        limittext = ' - Last ' + str(numdays) + ' days of APPLYMON data'\n",
    "        \n",
    "    # get a 2-dimensional object\n",
    "    ax = {}\n",
    "    \n",
    "    # print('DEBUG: qs=' + str(qs))\n",
    "    \n",
    "    # Loop, for each reqeive queue\n",
    "    for i in range(0,qs):\n",
    "        # print('DEBUG: i=' + str(i) + '; ' + lat_df_allq.recvq.unique()[i])\n",
    "\n",
    "        # First axes: Latency\n",
    "        \n",
    "        # old:\n",
    "        # plotpos = qs * 100 + 11 + i\n",
    "        # print('plotpos=' + str(plotpos))\n",
    "        # ax[i,0] = fig3.add_subplot(plotpos,title=lat_df_allq.recvq.unique()[i] + ' - Latency and throughput per hour')\n",
    "\n",
    "        # new:\n",
    "        # Axis position\n",
    "        #  First parameter: Number of plots = number of queues\n",
    "        #  Second parameter: Horizintal position or column (always 1 = only 1 column)\n",
    "        #  Third parameter: Vertical position or row (1, 2, 3, ... depending on the loop)\n",
    "        ax[i,0] = fig3.add_subplot(qs,1,i+1)\n",
    "\n",
    "        # Plot. Data frame limited to one queue per iteration\n",
    "        lat_df_allq[lat_df_allq.recvq == lat_df_allq.recvq.unique()[i]].plot(ax=ax[i,0], \n",
    "            x='monitor_date', \n",
    "            y=['max_apply_latency_sec','max_qlatency_sec','max_capture_latency_sec'],\n",
    "            kind='area', title=lat_df_allq.recvq.unique()[i] + ' APPLYMON statistics' + limittext)\n",
    "        \n",
    "        # monitor_date as xticks\n",
    "        plt.xticks(lat_df_allq.index,lat_df_allq['monitor_date'].values,rotation=70)\n",
    "\n",
    "        xticks1 = plt.gca().xaxis.get_major_ticks()\n",
    "        \n",
    "        # setting the tick density\n",
    "        if grouping == 'by_hour':\n",
    "            if (numdays > 5) or (numdays == -1):\n",
    "                if len(xticks1) < 100:\n",
    "                    tick_devider = 24\n",
    "                else:\n",
    "                    tick_devider = 48                \n",
    "            elif numdays > 2: \n",
    "                tick_devider = 4\n",
    "            else: \n",
    "                tick_devider = 2\n",
    "        else: \n",
    "            # by_day or by_monitor_interval\n",
    "            if len(xticks1) < 30:\n",
    "                tick_devider = 31\n",
    "            if len(xticks1) < 100:\n",
    "                tick_devider = 2\n",
    "            else:\n",
    "                tick_devider = 10\n",
    "        \n",
    "        for j in range(len(xticks1)):\n",
    "            if j % tick_devider != 0:\n",
    "                xticks1[j].set_visible(False)\n",
    "    \n",
    "        ax[i,0].set_xlabel('monitor date (yyyy-mm-dd-hh:mm)')\n",
    "        ax[i,0].set_ylabel('seconds')\n",
    "        ax[i,0].legend(loc='upper left')        \n",
    "        \n",
    "        # Second axes: Rows processed\n",
    "        ax[i,1] = ax[i,0].twinx()\n",
    "\n",
    "        # Plot. Data frame limited to one queue per iteration        \n",
    "        lat_df_allq[lat_df_allq.recvq == lat_df_allq.recvq.unique()[i]].plot(ax=ax[i,1], \n",
    "            x='monitor_date', \n",
    "            y='sum_rows_processed', \n",
    "            kind='line', linewidth=2, linestyle='-', color='m')\n",
    "\n",
    "        plt.ticklabel_format(style='plain', axis='y')\n",
    "\n",
    "        if grouping == 'by_hour':\n",
    "            ax[i,1].set_ylabel('rows processed per hour')\n",
    "        elif grouping == 'by_day':            \n",
    "            ax[i,1].set_ylabel('rows processed per day')\n",
    "        else:\n",
    "            ax[i,1].set_ylabel('rows processed')        \n",
    "            \n",
    "        ax[i,1].legend(loc='upper right')        \n",
    "\n",
    "    return(plt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read data from IBMQREP_APPLYMON, GROUPed BY DATE(MONITOR_DATE)\n",
    "# Returns a data frame\n",
    "\n",
    "def get_perf_applymon_day(conn):\n",
    "\n",
    "    # Default: testschema = ''  to read from default schema\n",
    "    # Set it to a specific test schema only for test purposes\n",
    "    testschema = ''\n",
    "    # testschema = 'IDH1I1A_20190509.'\n",
    "    \n",
    "    sql_text = \"select varchar(date(monitor_time)) as monitor_date, \\\n",
    "            recvq, \\\n",
    "            dec(max(capture_latency)) / 1000 as max_capture_latency_sec, \\\n",
    "            dec(max(qlatency)) / 1000 as max_qlatency_sec, \\\n",
    "            dec(max(apply_latency)) / 1000 as max_apply_latency_sec, \\\n",
    "            sum(rows_processed) as sum_rows_processed, \\\n",
    "            max(qdepth) as max_qdepth \\\n",
    "            from \" + testschema + \"ibmqrep_applymon \\\n",
    "            group by recvq, date(monitor_time) \\\n",
    "            order by date(monitor_time), recvq;\"\n",
    "   \n",
    "    try:\n",
    "        df = pandas.read_sql_query(sql_text, conn)\n",
    "    except exc.SQLAlchemyError as ex:\n",
    "        print('We got a problem......:' + str(ex)) \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read data from IBMQREP_APPLYMON, ungrouped\n",
    "# Returns a data frame\n",
    "\n",
    "def get_perf_applymon_detail(conn, start_ts):\n",
    "\n",
    "    # Default: testschema = ''  to read from default schema\n",
    "    # Set it to a specific test schema only for test purposes\n",
    "    testschema = ''\n",
    "    # testschema = 'BITMARCK_20190624.'\n",
    "    \n",
    "    if start_ts == 'max':\n",
    "        filterclause = \"WHERE MONITOR_TIME > (select max(monitor_time) - 1 hour \\\n",
    "                        from \" + testschema + \"ibmqrep_applymon)\"\n",
    "    else:\n",
    "        filterclause = \"WHERE MONITOR_TIME > timestamp('\" + start_ts + \"') \\\n",
    "                       AND MONITOR_TIME <= timestamp('\" + start_ts + \"') + 1 hour \"\n",
    "    \n",
    "    sql_text = \"select m.monitor_time, \\\n",
    "        m.recvq, \\\n",
    "        dec(m.capture_latency) / 1000 as capture_latency_sec, \\\n",
    "        dec(m.qlatency) / 1000 as qlatency_sec, \\\n",
    "        dec(m.apply_latency) / 1000 as apply_latency_sec, \\\n",
    "        dec(m.rows_processed) / (select dec(p.monitor_interval) / 1000 \\\n",
    "        from \" + testschema + \"ibmqrep_applyparms p ) as rows_processed_sec, \\\n",
    "        m.qdepth, \\\n",
    "        dec(m.mqget_time) / 1000 as mqget_time_sec, \\\n",
    "        dec(m.apply_sleep_time) / r.num_apply_agents / 1000 as apply_sleep_time_agent_sec, \\\n",
    "        dec(m.workq_wait_time) / 1000 as workq_wait_time_sec, \\\n",
    "        dec(m.dbms_time) / 1000 as dbms_time_sec, \\\n",
    "        dec(m.dependency_delay) / 1000 as  dependency_delay_sec\\\n",
    "        from \" + testschema + \"ibmqrep_applymon m, \" + testschema + \"ibmqrep_recvqueues r \\\n",
    "        \" + filterclause + \\\n",
    "        \" and m.recvq = r.recvq \\\n",
    "        order by m.monitor_time, m.recvq;\"\n",
    "\n",
    "    try:\n",
    "        df = pandas.read_sql_query(sql_text, conn)\n",
    "    except exc.SQLAlchemyError as ex:\n",
    "        print('We got a problem......:' + str(ex))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todo: xticks only for final plot. Legend misplaced.\n",
    "\n",
    "# Function to create a figure with n plots (one lot per receive queue) to display the \n",
    "# following values of IBMQREP_APPLYMON:\n",
    "#  - CAPTURE_LATENCY\n",
    "#  - APPLY_LATENCY\n",
    "#  - QLATENCY\n",
    "#  - ROWS_PROCESSED\n",
    "# Returns the plots\n",
    "\n",
    "def plot_lat_details(lat_df_allq,qs,start_ts,grouping):\n",
    "\n",
    "    if qs >=4:\n",
    "        calc_fig_height = numqs * 6\n",
    "    if qs >=2:\n",
    "        calc_fig_height = numqs * 8\n",
    "    else:\n",
    "        calc_fig_height = numqs * 10\n",
    "\n",
    "    \n",
    "    fig3 = plt.figure(1,figsize=(20,calc_fig_height))\n",
    "\n",
    "    limittext = \" - One hour of available data starting from '\" + start_ts + \"'\"\n",
    "        \n",
    "    # get a 2-dimensional object\n",
    "    ax = {}\n",
    "    \n",
    "    # print('DEBUG: qs=' + str(qs))\n",
    "    \n",
    "    # Loop, for each reqeive queue\n",
    "    for i in range(0,qs):\n",
    "        # print('DEBUG: i=' + str(i) + '; ' + lat_df_allq.recvq.unique()[i])\n",
    "\n",
    "        # First axes: Latency\n",
    "        \n",
    "        # old:\n",
    "        # plotpos = qs * 100 + 11 + i\n",
    "        # print('plotpos=' + str(plotpos))\n",
    "        # ax[i,0] = fig3.add_subplot(plotpos,title=lat_df_allq.recvq.unique()[i] + ' - Latency and throughput per hour')\n",
    "\n",
    "        # new:\n",
    "        # Axis position\n",
    "        #  First parameter: Number of plots = number of queues\n",
    "        #  Second parameter: Horizintal position or column (always 1 = only 1 column)\n",
    "        #  Third parameter: Vertical position or row (1, 2, 3, ... depending on the loop)\n",
    "        ax[i,0] = fig3.add_subplot(qs,1,i+1)\n",
    "\n",
    "        # Plot. Data frame limited to one queue per iteration\n",
    "        lat_df_allq[lat_df_allq.recvq == lat_df_allq.recvq.unique()[i]].plot(ax=ax[i,0], \n",
    "            x='monitor_time', \n",
    "            y=['apply_latency_sec','qlatency_sec','capture_latency_sec'],\n",
    "            kind='area', title=lat_df_allq.recvq.unique()[i] + ' APPLYMON statistics' + limittext)\n",
    "        \n",
    "        # monitor_time as xticks\n",
    "        plt.xticks(lat_df_allq.index,lat_df_allq['monitor_time'].values,rotation=70)\n",
    "\n",
    "        # only display 1 tick per day\n",
    "        xticks1 = plt.gca().xaxis.get_major_ticks()\n",
    "\n",
    "        if grouping == 'by_hour':\n",
    "            if (numdays > 5) or (numdays == -1):\n",
    "                if len(xticks1) < 100:\n",
    "                    tick_devider = 24\n",
    "                else:\n",
    "                    tick_devider = 48                \n",
    "            elif numdays > 2: \n",
    "                tick_devider = 4\n",
    "            else: \n",
    "                tick_devider = 2\n",
    "        else: \n",
    "            # by day or by_monitor_interval\n",
    "            if len(xticks1) < 30:\n",
    "                tick_devider = 31\n",
    "            if len(xticks1) < 100:\n",
    "                tick_devider = 2\n",
    "            else:\n",
    "                tick_devider = 10\n",
    "        \n",
    "        for j in range(len(xticks1)):\n",
    "            if j % tick_devider != 0:\n",
    "                xticks1[j].set_visible(False)\n",
    "    \n",
    "        ax[i,0].set_xlabel('monitor time (yyyy-mm-dd-hh:mm:ss)')\n",
    "        ax[i,0].set_ylabel('seconds')\n",
    "        ax[i,0].legend(loc='upper left')\n",
    "\n",
    "        # Second axes: Rows processed\n",
    "        ax[i,1] = ax[i,0].twinx()\n",
    "\n",
    "        # Plot. Data frame limited to one queue per iteration        \n",
    "        lat_df_allq[lat_df_allq.recvq == lat_df_allq.recvq.unique()[i]].plot(ax=ax[i,1], \n",
    "            x='monitor_time', \n",
    "            y=['rows_processed_sec','qdepth'], \n",
    "            kind='line', linewidth=2, linestyle='-', color=['magenta','slategrey'])\n",
    "\n",
    "        plt.ticklabel_format(style='plain', axis='y')\n",
    "        # ax[i,1].set_ylabel('rows processed per second')\n",
    "        ax[i,1].legend(loc='upper right')        \n",
    "        \n",
    "        # Third axes: timimg\n",
    "        ax[i,2] = ax[i,0].twinx()\n",
    "        ax[i,2].spines[\"right\"].set_position((\"axes\", 1.1))\n",
    "\n",
    "        # Plot. Data frame limited to one queue per iteration        \n",
    "        lat_df_allq[lat_df_allq.recvq == lat_df_allq.recvq.unique()[i]].plot(ax=ax[i,2], \n",
    "            x='monitor_time', \n",
    "            y=['mqget_time_sec','apply_sleep_time_agent_sec','workq_wait_time_sec','dependency_delay_sec'],\n",
    "            kind='line', linewidth=2, linestyle='--', color=['deepskyblue','orangered','indigo','yellow'])\n",
    "\n",
    "        plt.ticklabel_format(style='plain', axis='y')\n",
    "        ax[i,2].set_ylabel('seconds')\n",
    "        ax[i,2].legend(loc='center right')                \n",
    "\n",
    "    return(plt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q Apply Queue and Sub Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read data from IBMQREP_RECVQUEUES\n",
    "# Returns a data frame\n",
    "# The column list is not hard coded but retrieved from the catalog (all columns) to more flexibly\n",
    "# react on new control table columns\n",
    "\n",
    "def get_recvq_details(conn, schema):\n",
    "    cat_stmt = \"select name from sysibm.syscolumns \\\n",
    "                where tbcreator = '\" + schema + \"' \\\n",
    "                  and tbname = 'IBMQREP_RECVQUEUES' \\\n",
    "                order by COLNO;\"\n",
    "    \n",
    "    try:\n",
    "        cat_df = pandas.read_sql_query(cat_stmt, conn)\n",
    "    except exc.SQLAlchemyError as ex:\n",
    "        print('We got a problem......:' + str(ex))\n",
    "    \n",
    "    for index, row in cat_df.iterrows():\n",
    "        if index == 0:\n",
    "            collist = row['name']\n",
    "        else:\n",
    "            collist = collist + ', ' + row['name']\n",
    "        \n",
    "    sql_text = \"SELECT \" + collist + \" FROM IBMQREP_RECVQUEUES order by recvq;\"\n",
    "\n",
    "    \n",
    "    try:\n",
    "        df = pandas.read_sql_query(sql_text, conn)\n",
    "    except exc.SQLAlchemyError as ex:\n",
    "        print('We got a problem......:' + str(ex))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read data from IBMQREP_TARGETS\n",
    "# Returns a data frame\n",
    "# The column list is not hard coded but retrieved from the catalog (all columns) to more flexibly\n",
    "# react on new control table columns\n",
    "\n",
    "def get_targets_details(conn, schema):\n",
    "    cat_stmt = \"select name from sysibm.syscolumns \\\n",
    "                where tbcreator = '\" + schema + \"' \\\n",
    "                  and tbname = 'IBMQREP_TARGETS' \\\n",
    "                order by COLNO;\"\n",
    "    \n",
    "    try:\n",
    "        cat_df = pandas.read_sql_query(cat_stmt, conn)\n",
    "    except exc.SQLAlchemyError as ex:\n",
    "        print('We got a problem......:' + str(ex))\n",
    "    \n",
    "    for index, row in cat_df.iterrows():\n",
    "        if index == 0:\n",
    "            collist = row['name']\n",
    "        else:\n",
    "            collist = collist + ', ' + row['name']\n",
    "            \n",
    "    sql_text = \"SELECT \" + collist + \" FROM IBMQREP_TARGETS order by subname;\"    \n",
    "    try:\n",
    "        df = pandas.read_sql_query(sql_text, conn)\n",
    "    except exc.SQLAlchemyError as ex:\n",
    "        print('We got a problem......:' + str(ex))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q Apply Anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read the query for the Q Subscription check from \n",
    "# file (qrep_check_subs_apply.sql) and to execute the query. \n",
    "# Returns a data frame\n",
    "\n",
    "def get_apply_anomylies(conn):\n",
    "\n",
    "    if not os.path.isfile(qrep_check_qsubs_apply_file_name):\n",
    "        print('File does not exist.')\n",
    "    else:\n",
    "        with open(qrep_check_qsubs_apply_file_name , 'r') as f:\n",
    "            sql_text_list = f.read()\n",
    "\n",
    "    sql_text = ''.join(sql_text_list)        \n",
    "\n",
    "    f.close()\n",
    "\n",
    "    # debug\n",
    "    # print(sql_text)\n",
    "    \n",
    "    try:\n",
    "        df = pandas.read_sql_query(sql_text, conn)\n",
    "    except exc.SQLAlchemyError as ex:\n",
    "        print('We got a problem......:' + str(ex))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q Apply Control Table Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read data from IBMQREP_APPLYPARMS\n",
    "# Returns a data frame\n",
    "# The column list is not hard coded but retrieved from the catalog (all columns) to more flexibly\n",
    "# react on new control table columns\n",
    "\n",
    "def get_applyparms(conn, schema):\n",
    "    cat_stmt = \"select name from sysibm.syscolumns \\\n",
    "                where tbcreator = '\" + schema + \"' \\\n",
    "                  and tbname = 'IBMQREP_APPLYPARMS' \\\n",
    "                order by COLNO;\"\n",
    "    \n",
    "    try:\n",
    "        cat_df = pandas.read_sql_query(cat_stmt, conn)\n",
    "    except exc.SQLAlchemyError as ex:\n",
    "        print('We got a problem......:' + str(ex))\n",
    "    \n",
    "    for index, row in cat_df.iterrows():\n",
    "        if index == 0:\n",
    "            collist = row['name']\n",
    "        else:\n",
    "            collist = collist + ', ' + row['name']\n",
    "            \n",
    "    sql_text = \"SELECT \" + collist + \" FROM IBMQREP_APPLYPARMS;\"\n",
    "    \n",
    "    try:\n",
    "        df = pandas.read_sql_query(sql_text, conn)\n",
    "    except exc.SQLAlchemyError as ex:\n",
    "        print('We got a problem......:' + str(ex))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read data from IBMQREP_APPLYTRACE (only numrows)\n",
    "# Returns a data frame\n",
    "# The column list is not hard coded but retrieved from the catalog (all columns) to more flexibly\n",
    "# react on new control table columns\n",
    "\n",
    "def get_applytrace(conn, schema, numrows):\n",
    "    cat_stmt = \"select name from sysibm.syscolumns \\\n",
    "                where tbcreator = '\" + schema + \"' \\\n",
    "                  and tbname = 'IBMQREP_APPLYTRACE' \\\n",
    "                order by COLNO;\"\n",
    "    \n",
    "    try:\n",
    "        cat_df = pandas.read_sql_query(cat_stmt, conn)\n",
    "    except exc.SQLAlchemyError as ex:\n",
    "        print('We got a problem......:' + str(ex))\n",
    "    \n",
    "    for index, row in cat_df.iterrows():\n",
    "        if index == 0:\n",
    "            collist = row['name']\n",
    "        else:\n",
    "            collist = collist + ', ' + row['name']\n",
    "            \n",
    "    sql_text = \"SELECT \" + collist + \" FROM ibmqrep_applytrace order by trace_time desc \"\n",
    "\n",
    "    \n",
    "    if numrows > 0:\n",
    "        sql_text = sql_text + \" fetch first \" + str(numrows) + \" rows only;\"\n",
    "\n",
    "    try:\n",
    "        df = pandas.read_sql_query(sql_text, conn)\n",
    "    except exc.SQLAlchemyError as ex:\n",
    "        print('We got a problem......:' + str(ex))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read data from IBMQREP_APPLYMON with a WHERE clause respecting the \n",
    "# calculated data range. Returns a data frame\n",
    "# The column list is not hard coded but retrieved from the catalog (all columns) to more flexibly\n",
    "# react on new control table columns\n",
    "\n",
    "def get_applymon(conn, schema, start_date, end_date):\n",
    "    cat_stmt = \"select name from sysibm.syscolumns \\\n",
    "                where tbcreator = '\" + schema + \"' \\\n",
    "                  and tbname = 'IBMQREP_APPLYMON' \\\n",
    "                order by COLNO;\"\n",
    "    \n",
    "    try:\n",
    "        cat_df = pandas.read_sql_query(cat_stmt, conn)\n",
    "    except exc.SQLAlchemyError as ex:\n",
    "        print('We got a problem......:' + str(ex))\n",
    "\n",
    "    listOfHexCols = ['OLDEST_COMMIT_LSN' , 'OLDEST_COMMIT_SEQ']\n",
    "    for index, row in cat_df.iterrows():\n",
    "        if row['name'] in listOfHexCols:\n",
    "            colexp = \"hex(\" + row['name'] + \") as \" + row['name']\n",
    "        else: \n",
    "            colexp = row['name']\n",
    "\n",
    "        if index == 0:\n",
    "            collist = colexp\n",
    "        else:\n",
    "            collist = collist + ', ' + colexp\n",
    "            \n",
    "    sql_text = \"SELECT \" + collist + \" FROM IBMQREP_APPLYMON \\\n",
    "                where monitor_time >= '\" + start_date + \"' \\\n",
    "                  and  monitor_time <= '\" + end_date + \"' \\\n",
    "                order by monitor_time desc;\"\n",
    "    \n",
    "    try:\n",
    "        df = pandas.read_sql_query(sql_text, conn)\n",
    "    except exc.SQLAlchemyError as ex:\n",
    "        print('We got a problem......:' + str(ex))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"IBM Q Replication Monitoring Jupyter Library.ipynb loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Release Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>02.12.2019:</b> Global fix: 'connection' replaced by 'conn' for all invocations of read_sql_query(sql_text, conn).<br>\n",
    "<b>02.12.2019:</b> Some functions (e.g., 'get_sendq_details') just retrieve the content of IBMQREP control tables. Previously, these SQL queries contained a list of columns (all columns). This was changed for multiple queries to first generate the list of columns from the Db2 catalog and then build and execute the query. This increases the robustness of the Notebook because with that feature it supports multiple control table architecture levels (prevents errors because of missing (optional) columns).<p>\n",
    "<b>02.12.2019:</b>All \"print('We got a problem......:' + str(ex))\" changed to \"print('We got a problem......:' + str(ex))\" // ex cannot be concatenated to string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
